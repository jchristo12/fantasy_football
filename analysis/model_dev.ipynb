{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA & Model Development "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://raw.githubusercontent.com/jchristo12/fantasy_football/master/data/full_data.csv\n",
    "df = pd.read_csv('https://github.com/jchristo12/fantasy_football/blob/master/data/full_data.csv?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pk', 'gid', 'seas', 'wk', 'player', 'fname', 'lname', 'full_name',\n",
       "       'team', 'pos1',\n",
       "       ...\n",
       "       'ou', 'sprv', 'ptsv', 'ptsh', 'udog', 'gen_cond', 'udog_binary',\n",
       "       'gen_dv', 'def_team', 'f_pts'],\n",
       "      dtype='object', length=171)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows that have NaN for the shifted variables\n",
    "df_clean = df[~df.loc[:,'seas_pa':'seas_tdret'].isna().all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We are only focused on offensive players right now. Therefore, we will specifiy the positions we want to retain in the data and remove everyone else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#store positions we are concerned about; will use these to filter out \n",
    "pos_of_interest = ['QB', 'RB', 'WR', 'TE']\n",
    "#filter out positions we don't care about\n",
    "df_clean2 = df_clean[df_clean['pos1'].isin(pos_of_interest)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert each column to the appropriate data type (i.e. make the categorical data categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the column types\n",
    "col_dtypes = {'category': ['seas', 'wk', 'pos1', 'team', 'udog', 'v', 'h', 'day', 'stad', 'wdir',\n",
    "                          'surf', 'gen_cond', 'gen_dv', 'def_team']}\n",
    "#flip the key and values around so they will work in the argument for 'astype()'\n",
    "col_dtypes_alt = {old: new for new, old_all in col_dtypes.items() for old in old_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean2 = df_clean2.astype(col_dtypes_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove rookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store dataframe of non-rookies\n",
    "df_clean2_vet = df_clean2.loc[df_clean2['exp']!=1, :]\n",
    "#store dataframe of rookie data\n",
    "df_clean2_rook = df_clean2.loc[df_clean2['exp']==1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Segment out the WR data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-rookies data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr = df_clean2_vet.loc[df_clean2_vet['pos1']=='WR', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only focus on week 10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr10 = df_wr.loc[df_wr['wk'] == 10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1860, 171)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wr10.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only focusing on the WR data right now. Will need to build this so that each action is extendable to the other positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the random seed for reproducability\n",
    "random.seed(837)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break out the data between training and test\n",
    "train_wr, test_wr = train_test_split(df_wr10, train_size=0.75, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1395, 171) (465, 171)\n"
     ]
    }
   ],
   "source": [
    "#shape of the data\n",
    "print(train_wr.shape, test_wr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index on both dataframes\n",
    "train_wr = train_wr.reset_index(drop=True)\n",
    "test_wr = test_wr.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a series of percent of missing data\n",
    "missing_data_pct = train_wr.isna().sum() / train_wr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bench       0.718996\n",
       "hand        0.629391\n",
       "arm         0.612186\n",
       "cone        0.523297\n",
       "shuttle     0.509677\n",
       "broad       0.482437\n",
       "vertical    0.440860\n",
       "forty       0.376344\n",
       "dtype: float64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list all columns with missing data that is greater than 25%\n",
    "missing_data_pct[missing_data_pct > 0.25].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp    0.153405\n",
       "humd    0.033692\n",
       "wdir    0.025090\n",
       "dtype: float64"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#columns with missing values but less than or equal to 25%\n",
    "impute_cols = missing_data_pct[(missing_data_pct <= 0.25) & (missing_data_pct > 0)].sort_values(ascending=False)\n",
    "impute_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take action on missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop columns with too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns with missing data greater than 25%\n",
    "#store the column names\n",
    "missing_cols_del = missing_data_pct[missing_data_pct > 0.25].sort_values(ascending=False).index\n",
    "#drop the columns and store as new dataframe\n",
    "train_wr_miss = train_wr.drop(missing_cols_del, axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1395, 163)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape\n",
    "train_wr_miss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impute the rest of the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build simple imputers for both numeric and categorical features\n",
    "numeric_impute = SimpleImputer(missing_values=np.NaN, strategy='median')\n",
    "cat_impute = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of all of the features to impute\n",
    "missing_values_df = train_wr_miss.drop(train_wr_miss.columns.difference(impute_cols.index), axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['temp', 'wdir', 'humd'], dtype='object')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the columns that need to use the numeric imputation and the categorical imputation\n",
    "impute_numeric_col = missing_values_df.select_dtypes(include=np.number).columns\n",
    "impute_cat_col = missing_values_df.select_dtypes(exclude=np.number).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below attempts to handle infinity values; This is not an issue right now but something to be aware of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boolean if the column has 'inf' values or not\n",
    "#inf_cols = np.isinf(train_wr_miss.loc[:, impute_numeric_col]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns that have 'inf' values\n",
    "#inf_cols2 = list(train_wr_miss.loc[:, impute_numeric_col].columns.to_series()[inf_cols].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with simple imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute numeric columns\n",
    "imputed_numeric_df = pd.DataFrame(numeric_impute.fit_transform(train_wr_miss.loc[:, impute_numeric_col]), columns=impute_numeric_col).add_prefix('imp_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute categorical columns\n",
    "imputed_cat_df = pd.DataFrame(cat_impute.fit_transform(train_wr_miss.loc[:, impute_cat_col]), columns=impute_cat_col).add_prefix('imp_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add imputed columns to original data frame\n",
    "train_wr_imputed = pd.concat([train_wr_miss, imputed_numeric_df, imputed_cat_df], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
